"""
我们最开始设计了RLVR实验，经过20步的训练发现：模型在一开始几乎会裁掉95%以上的token
这是因为我们动了tokenizer词表的大小，即embedding层与LM head 层的分布有较大的变化，roulout变得极其不稳定，policy的概率分布远远脱离了原始模型的分布。
所以，这个策略是很难训练下去的。

为此，我们意识到，需要训一个冷启动模型。
于是，我们造了一批数据，进行了如下实验：
    1. 训练lora
        训练lora是因为：我们需要添加的新词还挺多，如果直接SFT训整个模型，无法快速的让embedding层和lm head层适配收敛到新的分布上。我们的数据非常有限，所以直接SFT方法是不可行的。
        因此，我们选择lora，在embedding层、lm head层、第一层的attention、和紧接lm head的attention层开启了训练，其余层冻结。
        我们一共有200条左右的样本，我们选择拿出100条的样本，进行数据增强！具体来说，我们数据增强的策略是在生成这200条样本的过程中，采用不同的t、top-k、top-p来采样数据得到。
        最终经过human review之后 剩下800左右 的样本量。

        我们拿200的数据来做冷启动实验，找了3个人，为我打标签。
        batch=2，gradient accumulation=4，即total_batch_size=8，在一张卡上进行训练。训练了84个步长（3个epoch）
        我们发现，loss从4.0+收敛到0.4左右不再下降，所以说，对新token和我们任务的适配度还是很不错的。

        但随之而来的又有一个新的问题，就是我在思考，"一个好的cold start model"难道真的适配到新任务就可以了吗？
        答案并不是这样，我们在通用数据集和数学数据集上对上述训练的cold model做了评估，令人遗憾的是
        MMLU-PRO : 69.6 -> 62.1
        GPQA: 62.0 -> 50.5
        其他的数据集没评估，但单单从这两个数据集来看，这并不是一个好的cold start model

        为了保证通用能力，按照经验来看，我们混入domain数据5倍数据量的通用数据+domain数据2倍数据量的数学能力的数据集进行训练
        总数据量=1600 训练了400+步长，loss从3.8+收敛到0.2 不再下降，结果一切正常！
    2. 训练SFT
        这个SFT实验是一开始做的，在lora之前，我们尝试了一下，不太可行。
        但是，我们也有在思考：既然直接训练不行，那么可以先冻结全部参数，只全参训练embedding和lm head层。
        然后再全参训练整个模型，这样是否可行？后续没再进行实验，但是一个值得思考的问题！性能是否会进一步提升？
"""